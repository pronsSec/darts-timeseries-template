{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Darts TimeSeries Template.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQ7jlLIE/o5stecu0qKm1b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pronsSec/darts-timeseries-template/blob/main/Darts_TimeSeries_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://unit8co.github.io/darts/quickstart/00-quickstart.html"
      ],
      "metadata": {
        "id": "afBPcU7CGr0N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk39GE7t-DSp"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.4.1\n",
        "!pip install darts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "#%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from darts import TimeSeries\n",
        "from darts.datasets import AirPassengersDataset"
      ],
      "metadata": {
        "id": "k0VkHtDz-IxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series = AirPassengersDataset().load()\n",
        "series.plot()"
      ],
      "metadata": {
        "id": "4jDwpyMkBDGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting\n",
        "series1, series2 = series.split_before(0.75)\n",
        "series1.plot()\n",
        "series2.plot()"
      ],
      "metadata": {
        "id": "YvNN0OG3Bafk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#slicing\n",
        "series1, series2 = series[:-36], series[-36:]\n",
        "series1.plot()\n",
        "series2.plot()"
      ],
      "metadata": {
        "id": "ID3-1SbBBgpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mapping\n",
        "series.map(np.log).plot()"
      ],
      "metadata": {
        "id": "DliIm3VCCD1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#diffing\n",
        "series.diff().plot()"
      ],
      "metadata": {
        "id": "rxRahRs9CTxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling missing values\n",
        "from darts.utils.missing_values import fill_missing_values\n",
        "\n",
        "values = np.arange(50, step=0.5)\n",
        "values[10:30] = np.nan\n",
        "values[60:95] = np.nan\n",
        "series_ = TimeSeries.from_values(values)\n",
        "\n",
        "(series_ - 10).plot(label=\"with missing values (shifted below)\")\n",
        "fill_missing_values(series_).plot(label=\"without missing values\")"
      ],
      "metadata": {
        "id": "yWg7yfiNCh3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating and plotting training and validation series'\n",
        "\n",
        "train, val = series.split_before(pd.Timestamp(\"19580101\"))\n",
        "train.plot(label=\"training\")\n",
        "val.plot(label=\"validation\")"
      ],
      "metadata": {
        "id": "P4xgKjX8CiCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#naive baseline model for prediction\n",
        "\n",
        "from darts.models import NaiveSeasonal\n",
        "\n",
        "naive_model = NaiveSeasonal(K=1)\n",
        "naive_model.fit(train)\n",
        "naive_forecast = naive_model.predict(36)\n",
        "\n",
        "series.plot(label=\"actual\")\n",
        "naive_forecast.plot(label=\"naive forecast (K=1)\")"
      ],
      "metadata": {
        "id": "pPoiIxlVCiFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inspect and confirm seasonality . there is a spike at 12 indicating yearly from our auto correlation function\n",
        "from darts.utils.statistics import plot_acf, check_seasonality\n",
        "\n",
        "plot_acf(train, m=12, alpha=0.05)"
      ],
      "metadata": {
        "id": "Ghl2o1Y6CiI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#naiveseasonal model again but with seasonality of 12 ... still missing the trend\n",
        "seasonal_model = NaiveSeasonal(K=12)\n",
        "seasonal_model.fit(train)\n",
        "seasonal_forecast = seasonal_model.predict(36)\n",
        "\n",
        "series.plot(label=\"actual\")\n",
        "seasonal_forecast.plot(label=\"naive forecast (K=12)\")"
      ],
      "metadata": {
        "id": "t_uU-SHqCiMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is better, but we are still missing the trend. Fortunately, there is also another naive baseline model capturing the trend, which is called NaiveDrift. This model simply produces linear predictions, with a slope that is determined by the first and last values of the training set:"
      ],
      "metadata": {
        "id": "u0GrPUjcEFus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import NaiveDrift\n",
        "\n",
        "drift_model = NaiveDrift()\n",
        "drift_model.fit(train)\n",
        "drift_forecast = drift_model.predict(36)\n",
        "\n",
        "combined_forecast = drift_forecast + seasonal_forecast - train.last_value()\n",
        "\n",
        "series.plot()\n",
        "combined_forecast.plot(label=\"combined\")\n",
        "drift_forecast.plot(label=\"drift\")"
      ],
      "metadata": {
        "id": "iBNkuaIPEG8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking errors with Mape (mean absolute percentage error) ... ironically dependency errors break this currently ...there are other ways to do it \n",
        "\n",
        "from darts.metrics import mape\n",
        "\n",
        "print(\n",
        "    \"Mean absolute percentage error for the combined naive drift + seasonal: {:.2f}%.\".format(\n",
        "        mape(series, combined_forecast)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "WXJ-DOGVEHDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#better way to train and validate several models with no error \n",
        "from darts.models import ExponentialSmoothing, Prophet, AutoARIMA, Theta\n",
        "\n",
        "\n",
        "def eval_model(model):\n",
        "    model.fit(train)\n",
        "    forecast = model.predict(len(val))\n",
        "    print(\"model {} obtains MAPE: {:.2f}%\".format(model, mape(val, forecast)))\n",
        "\n",
        "\n",
        "eval_model(ExponentialSmoothing())\n",
        "eval_model(Prophet())\n",
        "eval_model(AutoARIMA())\n",
        "eval_model(Theta())"
      ],
      "metadata": {
        "id": "SX4qQa4nEHGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "search for hyper parameters with theta method"
      ],
      "metadata": {
        "id": "a2RkuAK_FUB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for the best theta parameter, by trying 50 different values\n",
        "thetas = 2 - np.linspace(-10, 10, 50)\n",
        "\n",
        "best_mape = float(\"inf\")\n",
        "best_theta = 0\n",
        "\n",
        "for theta in thetas:\n",
        "    model = Theta(theta)\n",
        "    model.fit(train)\n",
        "    pred_theta = model.predict(len(val))\n",
        "    res = mape(val, pred_theta)\n",
        "\n",
        "    if res < best_mape:\n",
        "        best_mape = res\n",
        "        best_theta = theta"
      ],
      "metadata": {
        "id": "dzMzv2PtEHJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_theta_model = Theta(best_theta)\n",
        "best_theta_model.fit(train)\n",
        "pred_best_theta = best_theta_model.predict(len(val))\n",
        "\n",
        "print(\n",
        "    \"The MAPE is: {:.2f}, with theta = {}.\".format(\n",
        "        mape(val, pred_best_theta), best_theta\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "LSPX3F5xEHLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.plot(label=\"train\")\n",
        "val.plot(label=\"true\")\n",
        "pred_best_theta.plot(label=\"prediction\")"
      ],
      "metadata": {
        "id": "dXiMVBj0FPjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backtesting to simulate historical forecasting for validation\n"
      ],
      "metadata": {
        "id": "KeboMaqdFk9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historical_fcast_theta = best_theta_model.historical_forecasts(\n",
        "    series, start=0.6, forecast_horizon=3, verbose=True\n",
        ")\n",
        "\n",
        "series.plot(label=\"data\")\n",
        "historical_fcast_theta.plot(label=\"backtest 3-months ahead forecast (Theta)\")\n",
        "print(\"MAPE = {:.2f}%\".format(mape(historical_fcast_theta, series)))"
      ],
      "metadata": {
        "id": "c7fxALL5Flb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it appears to be overfit, using backtest() to obtain raw mape errors \n",
        "best_theta_model = Theta(best_theta)\n",
        "\n",
        "raw_errors = best_theta_model.backtest(\n",
        "    series, start=0.6, forecast_horizon=3, metric=mape, reduction=None, verbose=True\n",
        ")\n",
        "\n",
        "from darts.utils.statistics import plot_hist\n",
        "\n",
        "plot_hist(\n",
        "    raw_errors,\n",
        "    bins=np.arange(0, max(raw_errors), 1),\n",
        "    title=\"Individual backtest error scores (histogram)\",\n",
        ")"
      ],
      "metadata": {
        "id": "EJfwD6b3Fle8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_error = best_theta_model.backtest(\n",
        "    series,\n",
        "    start=0.6,\n",
        "    forecast_horizon=3,\n",
        "    metric=mape,\n",
        "    reduction=np.mean,  # this is actually the default\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"Average error (MAPE) over all historical forecasts: %.2f\" % average_error)"
      ],
      "metadata": {
        "id": "WkDMARRNFlhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at fitted value residuals of theta model.. basically the dif between 1 step forecasts"
      ],
      "metadata": {
        "id": "f_3oB7ImGXyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.utils.statistics import plot_residuals_analysis\n",
        "\n",
        "plot_residuals_analysis(best_theta_model.residuals(series))"
      ],
      "metadata": {
        "id": "fhAw8KmRFPv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the distribution is not centered at 0, which means that our Theta model is biased. We can also make out a large ACF value at lag equal to 12, which indicates that the residuals contain information that was not used by the model.\n",
        "\n",
        "Could we maybe do better with a simple ExponentialSmoothing model?"
      ],
      "metadata": {
        "id": "ISUcn5LRG125"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_es = ExponentialSmoothing()\n",
        "historical_fcast_es = model_es.historical_forecasts(\n",
        "    series, start=0.6, forecast_horizon=3, verbose=True\n",
        ")\n",
        "\n",
        "series.plot(label=\"data\")\n",
        "historical_fcast_es.plot(label=\"backtest 3-months ahead forecast (Exp. Smoothing)\")\n",
        "print(\"MAPE = {:.2f}%\".format(mape(historical_fcast_es, series)))"
      ],
      "metadata": {
        "id": "nnF6ulBtFP46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_residuals_analysis(model_es.residuals(series))"
      ],
      "metadata": {
        "id": "0Mik5900GwhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "op93YIJ8Gwpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The residual analysis also reflects an improved performance in that we now have a distribution of the residuals centred at value 0, and the ACF values, although not insignificant, have lower magnitudes.\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Machine learning and global models\n",
        "Darts has a rich support for machine learning and deep learning forecasting models; for instance:\n",
        "\n",
        "RegressionModel can wrap around any sklearn-compatible regression model to produce forecasts (it has its own section below).\n",
        "\n",
        "RNNModel is a flexible RNN implementation, which can be used like DeepAR.\n",
        "\n",
        "NBEATSModel implements the N-BEATS model.\n",
        "\n",
        "TFTModel implements the Temporal Fusion Transformer model.\n",
        "\n",
        "TCNModel implements temporal convolutional networks.\n",
        "\n",
        "…\n",
        "\n",
        "In addition to supporting the same basic fit()/predict() interface as the other models, these models are also global models, as they support being trained on multiple time series (sometimes referred to as meta learning).\n",
        "\n",
        "This is a key point of using ML-based models for forecasting: more often than not, ML models (especially deep learning models) need to be trained on large amounts of data, which often means a large amount of separate yet related time series.\n",
        "\n",
        "In Darts, the basic way to specify multiple TimeSeries is using a Sequence of TimeSeries (for instance, a simple list of TimeSeries)."
      ],
      "metadata": {
        "id": "VQphmA0hHN36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "A toy example with two series\n",
        "These models can be trained on thousands of series. Here, for the sake of illustration, we will load two distinct series - the air traffic passenger count and another series containing the number of pounds of milk produced per cow monthly. We also cast our series to np.float32 as that will slightly speedup the training:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fYhUCwE8IOSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.datasets import AirPassengersDataset, MonthlyMilkDataset\n",
        "\n",
        "series_air = AirPassengersDataset().load().astype(np.float32)\n",
        "series_milk = MonthlyMilkDataset().load().astype(np.float32)\n",
        "\n",
        "# set aside last 36 months of each series as validation set:\n",
        "train_air, val_air = series_air[:-36], series_air[-36:]\n",
        "train_milk, val_milk = series_milk[:-36], series_milk[-36:]\n",
        "\n",
        "train_air.plot()\n",
        "val_air.plot()\n",
        "train_milk.plot()\n",
        "val_milk.plot()"
      ],
      "metadata": {
        "id": "TmywjPPsGwsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sclaing between 0-1\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "\n",
        "scaler = Scaler()\n",
        "train_air_scaled, train_milk_scaled = scaler.fit_transform([train_air, train_milk])\n",
        "\n",
        "train_air_scaled.plot()\n",
        "train_milk_scaled.plot()"
      ],
      "metadata": {
        "id": "d2znGAFGGwvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "N beats model\n",
        "\n",
        "Using deep learning: example with N-BEATS\n",
        "Next, we will build an N-BEATS model. This model can be tuned with many hyper-parameters (such as number of stacks, layers, etc). Here, for simplicity, we will use it with default hyper-parameters. The only two hyper-parameters that we have to provide are:\n",
        "\n",
        "input_chunk_length: this is the “lookback window” of the model - i.e., how many time steps of history the neural network takes as input to produce its output in a forward pass.\n",
        "\n",
        "output_chunk_length: this is the “forward window” of the model - i.e., how many time steps of future values the neural network outputs in a forward pass.\n",
        "\n",
        "The random_state parameter is just here to get reproducible results.\n",
        "\n",
        "Most neural networks in Darts require these two parameters. Here, we will use multiples of the seasonality. We are now ready to fit our model on our two series (by giving a list containing the two series to fit()):"
      ],
      "metadata": {
        "id": "OGke3sNYIf3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import NBEATSModel\n",
        "\n",
        "model = NBEATSModel(input_chunk_length=24, output_chunk_length=12, random_state=42)\n",
        "\n",
        "model.fit([train_air_scaled, train_milk_scaled], epochs=50, verbose=True)"
      ],
      "metadata": {
        "id": "5dKhB-alGwyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now get some forecasts 36 months ahead, for our two series. We can just use the series argument of the fit() function to tell the model which series to forecast. Importantly, the output_chunk_length does not directly constrain the forecast horizon n that can be used with predict(). Here, we trained the model with output_chunk_length=12 and produce forecasts for n=36 months ahead; this is simply done in an auto-regressive way behind the scenes (where the network recursively consumes its previous outputs)."
      ],
      "metadata": {
        "id": "NCWXc5y0IxE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_air = model.predict(series=train_air_scaled, n=36)\n",
        "pred_milk = model.predict(series=train_milk_scaled, n=36)\n",
        "\n",
        "# scale back:\n",
        "pred_air, pred_milk = scaler.inverse_transform([pred_air, pred_milk])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "series_air.plot(label=\"actual (air)\")\n",
        "series_milk.plot(label=\"actual (milk)\")\n",
        "pred_air.plot(label=\"forecast (air)\")\n",
        "pred_milk.plot(label=\"forecast (milk)\")"
      ],
      "metadata": {
        "id": "oU7HB1sRIoAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Covariates: using external data\n",
        "In addition to the target series (the series we are interested to forecast), many models in Darts also accept covariates series in input. Covariates are series that we do not want to forecast, but which can provide helpful additional information to the models. Both the targets and covariates can be multivariate or univariate.\n",
        "\n",
        "There are two kinds of covariate time series in Darts:\n",
        "\n",
        "past_covariates are series not necessarily known ahead of the forecast time. Those can for instance represent things that have to be measured and are not known upfront. Models do not use the future values of past_covariates when making forecasts.\n",
        "\n",
        "future_covariates are series which are known in advance, up to the forecast horizon. This can represent things such as calendar information, holidays, weather forecasts, etc. Models that accept future_covariates will look at the future values (up to the forecast horizon) when making forecasts.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Each covariate can potentially be multivariate. If you have several covariate series (such as month and year values), you should stack() or concatenate() them to obtain a multivariate series.\n",
        "\n",
        "The covariates you provide can be longer than necessary. Darts will try to be smart and slice them in the right way for forecasting the target, based on the time indexes of the different series. You will receive an error if your covariates do not have a sufficient time span, though.\n",
        "\n",
        "Let’s now build some external covariates containing both monthly and yearly values for our air and milk series. In the cell below, we use the darts.utils.timeseries_generation.datetime_attribute_timeseries() function to generate series containing the month and year values, and we concatenate() these series along the \"component\" axis in order to obtain one covariate series with two components (month and year), per target series. For simplicity, we directly scale the month and year values to have them between (roughly) 0 and 1:"
      ],
      "metadata": {
        "id": "8L7MDMRRJFHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#xarray error \n",
        "from darts import concatenate\n",
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries as dt_attr\n",
        "\n",
        "air_covs = concatenate(\n",
        "    [\n",
        "        dt_attr(series_air.time_index, \"month\", dtype=np.float32) / 12,\n",
        "        (dt_attr(series_air.time_index, \"year\", dtype=np.float32) - 1948) / 12,\n",
        "    ],\n",
        "    axis=\"component\",\n",
        ")\n",
        "\n",
        "milk_covs = concatenate(\n",
        "    [\n",
        "        dt_attr(series_milk.time_index, \"month\", dtype=np.float32) / 12,\n",
        "        (dt_attr(series_milk.time_index, \"year\", dtype=np.float32) - 1962) / 13,\n",
        "    ],\n",
        "    axis=\"component\",\n",
        ")\n",
        "\n",
        "air_covs.plot()\n",
        "plt.title(\n",
        "    \"one multivariate time series of 2 dimensions, containing covariates for the air series:\"\n",
        ");"
      ],
      "metadata": {
        "id": "NoXP-hISIoC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NBEATSModel(input_chunk_length=24, output_chunk_length=12, random_state=42)\n",
        "\n",
        "model.fit(\n",
        "    [train_air_scaled, train_milk_scaled],\n",
        "    past_covariates=[air_covs, milk_covs],\n",
        "    epochs=50,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "yUBcQl2oIoF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_air = model.predict(series=train_air_scaled, past_covariates=air_covs, n=36)\n",
        "pred_milk = model.predict(series=train_milk_scaled, past_covariates=milk_covs, n=36)\n",
        "\n",
        "# scale back:\n",
        "pred_air, pred_milk = scaler.inverse_transform([pred_air, pred_milk])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "series_air.plot(label=\"actual (air)\")\n",
        "series_milk.plot(label=\"actual (milk)\")\n",
        "pred_air.plot(label=\"forecast (air)\")\n",
        "pred_milk.plot(label=\"forecast (milk)\")"
      ],
      "metadata": {
        "id": "xpmomlmhIoJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fT5k6p-JJiuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoders = {\n",
        "    \"cyclic\": {\"future\": [\"month\"]},\n",
        "    \"datetime_attribute\": {\"future\": [\"hour\", \"dayofweek\"]},\n",
        "    \"position\": {\"past\": [\"absolute\"], \"future\": [\"relative\"]},\n",
        "    \"custom\": {\"past\": [lambda idx: (idx.year - 1950) / 50]},\n",
        "    \"transformer\": Scaler(),\n",
        "}"
      ],
      "metadata": {
        "id": "T_akyrMaJY_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoders = {\"datetime_attribute\": {\"past\": [\"month\", \"year\"]}, \"transformer\": Scaler()}\n"
      ],
      "metadata": {
        "id": "J2r0O5_BJZBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NBEATSModel(\n",
        "    input_chunk_length=24,\n",
        "    output_chunk_length=12,\n",
        "    add_encoders=encoders,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "model.fit([train_air_scaled, train_milk_scaled], epochs=50, verbose=True)"
      ],
      "metadata": {
        "id": "nnORjonMJZEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_air = model.predict(series=train_air_scaled, n=36)\n",
        "\n",
        "# scale back:\n",
        "pred_air = scaler.inverse_transform(pred_air)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "series_air.plot(label=\"actual (air)\")\n",
        "pred_air.plot(label=\"forecast (air)\")"
      ],
      "metadata": {
        "id": "p_9fZdAYJZG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression forecasting models\n",
        "RegressionModel’s are forecasting models which wrap around sklearn-compatible regression models. The inner regression model is used to predict future values of the target series, as a function of certain lags of the target, past and future covariates. Behind the scenes, the time series are tabularized in order to build a training dataset in the right format.\n",
        "\n",
        "By default, the RegressionModel will do a linear regression. It is very easy to use any desired sklearn-compatible regression model by specifying the model parameter, but for convenience Darts also provides a couple of ready-made models out of the box:\n",
        "\n",
        "RandomForest wraps around sklearn.ensemble.RandomForestRegressor.\n",
        "\n",
        "LightGBMModel wraps around lightbm.\n",
        "\n",
        "LinearRegressionModel wraps around sklearn.linear_model.LinearRegression (accepting the same kwargs).\n",
        "\n",
        "For example, this is what fitting a Bayesian ridge regression to our toy two-series problem looks like:"
      ],
      "metadata": {
        "id": "wUp0K9lqNhgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#these are broken by xarray .. will revisit .. other tools do this also \n",
        "\n",
        "from darts.models import RegressionModel\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "model = RegressionModel(lags=72, lags_future_covariates=[-6, 0], model=BayesianRidge())\n",
        "\n",
        "model.fit(\n",
        "    [train_air_scaled, train_milk_scaled], future_covariates=[air_covs, milk_covs]\n",
        ")"
      ],
      "metadata": {
        "id": "ubq4qpZsJZJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Probabilistic forecasts\n",
        "Some models can produce probabilistic forecasts. This is the case for all deep learning models (such as RNNModel, NBEATSModel, etc …), as well as for ARIMA and ExponentialSmoothing. The full list is available on the Darts README page.\n",
        "\n",
        "For ARIMA and ExponentialSmoothing, one can simply specify a num_samples parameter to the predict() function. The returned TimeSeries will then be composed of num_samples Monte Carlo samples describing the distribution of the time series’ values. The advantage of relying on Monte Carlo samples (in contrast to, say, explicit confidence intervals) is that they can be used to describe any parametric or non-parametric joint distribution over components, and compute arbitrary quantiles.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9oud6kLcNzpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_es = ExponentialSmoothing()\n",
        "model_es.fit(train)\n",
        "probabilistic_forecast = model_es.predict(len(val), num_samples=500)\n",
        "\n",
        "series.plot(label=\"actual\")\n",
        "probabilistic_forecast.plot(label=\"probabilistic forecast\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "foEJ43FpNkzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With neural networks\n",
        "With neural networks, one has to give a Likelihood object to the model. The likelihoods specify which distribution the model will try to fit, along with potential prior values for the distributions’ parameters. The full list of available likelihoods is available in the docs.\n",
        "\n",
        "Using likelihoods is easy. For instance, here is what training an NBEATSModel to fit a Laplace likelihood looks like:"
      ],
      "metadata": {
        "id": "85_B_yPsN9YV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import TCNModel\n",
        "from darts.utils.likelihood_models import LaplaceLikelihood\n",
        "\n",
        "model = TCNModel(\n",
        "    input_chunk_length=24,\n",
        "    output_chunk_length=12,\n",
        "    random_state=42,\n",
        "    likelihood=LaplaceLikelihood(),\n",
        ")\n",
        "\n",
        "model.fit(train_air_scaled, epochs=400, verbose=True)"
      ],
      "metadata": {
        "id": "DsXyQ6TwNk2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(n=36, num_samples=500)\n",
        "\n",
        "# scale back:\n",
        "pred = scaler.inverse_transform(pred)\n",
        "\n",
        "series_air.plot()\n",
        "pred.plot()"
      ],
      "metadata": {
        "id": "xZNkkSzxNk65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Types of distributions\n",
        "The likelihood has to be compatible with the domain of your time series’ values. For instance PoissonLikelihood can be used on discrete positive values, ExponentialLikelihood can be used on real positive values, and BetaLikelihood on real values in .\n",
        "\n",
        "It is also possible to use QuantileRegression to apply a quantile loss and fit some desired quantiles directly.\n",
        "---\n",
        "\n",
        "\n",
        "Evaluating Probabilistic Forecasts\n",
        "How can we evaluate the quality of probabilistic forecasts? By default, most metrics functions (such as mape()) will keep working but look only at the median forecast. It is also possible to use the -risk metric (or quantile loss), which quantifies the error for each predicted quantiles:"
      ],
      "metadata": {
        "id": "ArWaaG4cPC0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.metrics import rho_risk\n",
        "\n",
        "print(\"MAPE of median forecast: %.2f\" % mape(series_air, pred))\n",
        "for rho in [0.05, 0.1, 0.5, 0.9, 0.95]:\n",
        "    rr = rho_risk(series_air, pred, rho=rho)\n",
        "    print(\"rho-risk at quantile %.2f: %.2f\" % (rho, rr))"
      ],
      "metadata": {
        "id": "_acicJfOPEzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Using Quantile Loss¶\n",
        "Could we do better by fitting these quantiles directly? We can just use a QuantileRegression likelihood:\n"
      ],
      "metadata": {
        "id": "01TzwLUPO4TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.utils.likelihood_models import QuantileRegression\n",
        "\n",
        "model = TCNModel(\n",
        "    input_chunk_length=24,\n",
        "    output_chunk_length=12,\n",
        "    random_state=42,\n",
        "    likelihood=QuantileRegression([0.05, 0.1, 0.5, 0.9, 0.95]),\n",
        ")\n",
        "\n",
        "model.fit(train_air_scaled, epochs=400, verbose=True)"
      ],
      "metadata": {
        "id": "D1Ni1mxFNk97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(n=36, num_samples=500)\n",
        "\n",
        "# scale back:\n",
        "pred = scaler.inverse_transform(pred)\n",
        "\n",
        "series_air.plot()\n",
        "pred.plot()\n",
        "\n",
        "print(\"MAPE of median forecast: %.2f\" % mape(series_air, pred))\n",
        "for rho in [0.05, 0.1, 0.5, 0.9, 0.95]:\n",
        "    rr = rho_risk(series_air, pred, rho=rho)\n",
        "    print(\"rho-risk at quantile %.2f: %.2f\" % (rho, rr))"
      ],
      "metadata": {
        "id": "2-bmqb88NlBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Ensembling models\n",
        "Ensembling is about combining the forecasts produced by several models, in order to obtain a final - and hopefully better forecast.\n",
        "\n",
        "For instance, in our example of a less naive model above, we manually combined a naive seasonal model with a naive drift model. Here, we will show how models forecasts can be automatically combined, naively using a NaiveEnsembleModel, or learned using RegressionEnsembleModel.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Naive Ensembling\n",
        "Naive ensembling just takes the average of the forecasts of several models. Darts provides a NaiveEnsembleModel, which allows to do this while still manipulating only one forecasting model (which, for instance, allows for easier backtesting):\n"
      ],
      "metadata": {
        "id": "WCe28c9BPues"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is naive so it's gonna suck \n",
        "from darts.models import NaiveEnsembleModel\n",
        "\n",
        "models = [NaiveDrift(), NaiveSeasonal(12)]\n",
        "\n",
        "ensemble_model = NaiveEnsembleModel(models=models)\n",
        "\n",
        "backtest = ensemble_model.historical_forecasts(\n",
        "    series_air, start=0.6, forecast_horizon=3, verbose=True\n",
        ")\n",
        "\n",
        "print(\"MAPE = %.2f\" % (mape(backtest, series_air)))\n",
        "series_air.plot()\n",
        "backtest.plot()"
      ],
      "metadata": {
        "id": "iEmYyL0_JZMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learned Ensembling\n",
        "As expected in this case, the naive ensemble doesn’t give great results (although in some cases it could!)\n",
        "\n",
        "We can sometimes do better if we see the ensembling as a supervised regression problem: given a set of forecasts (features), find a model that combines them in order to minimise errors on the target. This is what the RegressionEnsembleModel does. It accepts three parameters:\n",
        "\n",
        "forecasting_models is a list of forecasting models whose predictions we want to ensemble.\n",
        "\n",
        "regression_train_n_points is the number of time steps to use for fitting the “ensemble regression” model (i.e., the inner model that combines the forecasts).\n",
        "\n",
        "regression_model is, optionally, a sklearn-compatible regression model or a Darts RegressionModel to be used for the ensemble regression. If not specified, a linear regression is used. Using a sklearn model is easy out-of-the-box, but using a RegressionModel allows to potentially take arbitrary lags of the individual forecasts as inputs of the regression model.\n",
        "\n",
        "Once these elements are in place, a RegressionEnsembleModel can be used like a regular forecasting model:"
      ],
      "metadata": {
        "id": "eLCCE_Q8P5lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import RegressionEnsembleModel\n",
        "\n",
        "models = [NaiveDrift(), NaiveSeasonal(12)]\n",
        "\n",
        "ensemble_model = RegressionEnsembleModel(\n",
        "    forecasting_models=models, regression_train_n_points=12\n",
        ")\n",
        "\n",
        "backtest = ensemble_model.historical_forecasts(\n",
        "    series_air, start=0.6, forecast_horizon=3, verbose=True\n",
        ")\n",
        "\n",
        "print(\"MAPE = %.2f\" % (mape(backtest, series_air)))\n",
        "series_air.plot()\n",
        "backtest.plot()\n",
        "#inspect coefficients ... unnecesary\n",
        "ensemble_model.regression_model.model.coef_"
      ],
      "metadata": {
        "id": "pvTzwNsQP947"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Filtering models\n",
        "In addition to forecasting models, which are able to predict future values of series, Darts also contains a couple of helpful filtering models, which can model “in sample” series’ values distributions.\n",
        "\n",
        "Fitting a Kalman Filter\n",
        "KalmanFilter implements a Kalman Filter. The implementation relies on nfoursid, so it is for instance possible to provide a nfoursid.kalman.Kalman object containing a transition matrix, process noise covariance, observation noise covariance etc.\n",
        "\n",
        "It is also possible to do system identification by calling fit() to “train” the Kalman Filter using the N4SID system identification algorithm:"
      ],
      "metadata": {
        "id": "66W16KYbQdHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import KalmanFilter\n",
        "\n",
        "kf = KalmanFilter(dim_x=3)\n",
        "kf.fit(train_air_scaled)\n",
        "filtered_series = kf.filter(train_air_scaled, num_samples=100)\n",
        "\n",
        "train_air_scaled.plot()\n",
        "filtered_series.plot()"
      ],
      "metadata": {
        "id": "JNlMu2AeP-B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Inferring missing values with Gaussian Processes¶\n",
        "Darts also contains a GaussianProcessFilter which can be used for probabilistic modeling of series:"
      ],
      "metadata": {
        "id": "f64KvWhkQk5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import GaussianProcessFilter\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "\n",
        "# create a series with holes:\n",
        "values = train_air_scaled.values()\n",
        "values[20:22] = np.nan\n",
        "values[28:32] = np.nan\n",
        "values[55:59] = np.nan\n",
        "values[72:80] = np.nan\n",
        "series_holes = TimeSeries.from_times_and_values(train_air_scaled.time_index, values)\n",
        "series_holes.plot()\n",
        "\n",
        "kernel = RBF()\n",
        "\n",
        "gpf = GaussianProcessFilter(kernel=kernel, alpha=0.1, normalize_y=True)\n",
        "filtered_series = gpf.filter(series_holes, num_samples=100)\n",
        "\n",
        "filtered_series.plot()"
      ],
      "metadata": {
        "id": "uRPiy5ycP-Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "A Word of Caution\n",
        "So is N-BEATS, exponential smoothing, or a Bayesian ridge regression trained on milk production the best approach for predicting the future number of airline passengers? Well, at this point it’s actually hard to say exactly which one is best. Our time series is small, and our validation set is even smaller. In such cases, it’s very easy to overfit the whole forecasting exercise to such a small validation set. That’s especially true if the number of available models and their degrees of freedom is high (such as for deep learning models), or if we played with many models on a single test set (as done in this notebook).\n",
        "\n",
        "As data scientists, it is our responsibility to understand the extent to which our models can be trusted. So always take results with a grain of salt, especially on small datasets, and apply the scientific method before making any kind of forecast :) Happy modeling!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JkDIGc6AQs6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@misc{herzen2021darts,\n",
        "      title={Darts: User-Friendly Modern Machine Learning for Time Series},\n",
        "      author={Julien Herzen and Francesco Lässig and Samuele Giuliano Piazzetta and Thomas Neuer and Léo Tafti and Guillaume Raille and Tomas Van Pottelbergh and Marek Pasieka and Andrzej Skrodzki and Nicolas Huguenin and Maxime Dumonal and Jan Kościsz and Dennis Bader and Frédérick Gusset and Mounir Benheddi and Camila Williamson and Michal Kosinski and Matej Petrik and Gaël Grosch},\n",
        "      year={2021},\n",
        "      eprint={2110.03224},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.LG}\n",
        "}"
      ],
      "metadata": {
        "id": "z_7sw1QQQz8b"
      }
    }
  ]
}